{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Np8icKBaCEeM"
   },
   "source": [
    "## Q1. Is there any way to combine five different models that have all been trained on the same training data and have all achieved 95 percent precision? If so, how can you go about doing it? If not, what is the reason?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LvGksCSlCAWU"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Yes, they can be combined using stacking technique.\n",
    "Or, we can create bag of different models and do prediction using VotingClassifier/Regressor from sklearn.ensemble\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ImsKEB3DNW_"
   },
   "source": [
    "## Q2. What's the difference between hard voting classifiers and soft voting classifiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubZk9XAXDOpo"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Hard Voting Clasifier:  Hard voting classifier classifies input data based on the mode of all the predictions made by different classifiers. \n",
    "\n",
    "Soft Voting Clasifier: Soft voting classifier classifies input data based on the probabilities of all the predictions made by different classifiers.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3dDyTTsD9Xt"
   },
   "source": [
    "## Q3. Is it possible to distribute a bagging ensemble's training through several servers to speed up the process? Pasting ensembles, boosting ensembles, Random Forests, and stacking ensembles are all options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lhtF4eWeD-lI"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "It is quite possible to speed up training of a bagging ensemble by distributing it across multiple servers, \n",
    "since each predictor in the ensemble is independent of the others. The same goes for pasting ensembles and Random Forests for the same reason. However, \n",
    "each predictor in a boosting ensemble is built based on the previous predictor, so training is necessarily sequential and you will not gain anything by \n",
    "distributing training across multiple servers. Regarding stacking ensembles, all the predictors in a given layer are independent of each other, \n",
    "so they can be trained parallel on multiple servers. However, the predictors in one layer can only be trained after the predictors in the previous layer have all been trained.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9MCFZlxE5pW"
   },
   "source": [
    "## Q4. What is the advantage of evaluating out of the bag?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zE2LMccEE6wL"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The advantage is that OOB score is calculated by using the trees in the ensemble that doesn’t have those specific data points, so with OOB, we are not using the full ensemble. \n",
    "In other words, for predicting each sample in OOB set, we only consider trees that did not use that sample to train themselves.\n",
    "Where as with a validation set you use your full forest to calculate the score. And in general a full forest is better than a subsets of a forest.\n",
    "Hence, on average, OOB score is showing less generalization than the validation score because we are using less trees to get the predictions for each sample.\n",
    "OOB particularly helps when we can’t afford to hold out a validation set.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlZe_ao-F_Am"
   },
   "source": [
    "## Q5. What distinguishes Extra-Trees from ordinary Random Forests? What good would this extra randomness do? Is it true that Extra-Tree Random Forests are slower or faster than normal Random Forests?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJ4HFKakGACy"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Diference:\n",
    "Extra-Trees uses entire dataset but Random Forest uses bootstraping. \n",
    "In order to split nodes. Random Forest chooses the optimum split while Extra Trees chooses it randomly.\n",
    "\n",
    "Extra randomness helps generalize better\n",
    "\n",
    "Extra Trees algorithm is faster, this algorithm saves time because the whole procedure is the same \n",
    "but it randomly chooses the split point and does not calculate the optimal one.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBqEAu0QKJp8"
   },
   "source": [
    "## Q6. Which hyperparameters and how do you tweak if your AdaBoost ensemble underfits the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4AFO0zn3KM1y"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "try increasing the number of estimators or reducing the regularization hyperparameters of the base estimator, also try slightly increasing the learning rate.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzZVEFHNKO1d"
   },
   "source": [
    "## Q7. Should you raise or decrease the learning rate if your Gradient Boosting ensemble overfits the training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8r94KLslKQNQ"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "decreasing the learning rate, early stopping to find the right number of predictors \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ML Assignment 22.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
